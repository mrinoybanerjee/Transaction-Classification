{"cells":[{"cell_type":"markdown","metadata":{},"source":["## BERT BASE"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-04-03T01:03:41.100027Z","iopub.status.busy":"2024-04-03T01:03:41.099767Z","iopub.status.idle":"2024-04-03T01:07:40.962357Z","shell.execute_reply":"2024-04-03T01:07:40.960993Z","shell.execute_reply.started":"2024-04-03T01:03:41.100005Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2645: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Initial Validation Accuracy: 0.0000\n","======== Epoch 1 / 25 ========\n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":[" Average training loss: 2.61\n","======== Epoch 2 / 25 ========\n"," Average training loss: 1.93\n","======== Epoch 3 / 25 ========\n"," Average training loss: 1.53\n","======== Epoch 4 / 25 ========\n"," Average training loss: 1.24\n","======== Epoch 5 / 25 ========\n"," Average training loss: 1.04\n","======== Epoch 6 / 25 ========\n"," Average training loss: 0.91\n","======== Epoch 7 / 25 ========\n"," Average training loss: 0.80\n","======== Epoch 8 / 25 ========\n"," Average training loss: 0.74\n","======== Epoch 9 / 25 ========\n"," Average training loss: 0.66\n","======== Epoch 10 / 25 ========\n"," Average training loss: 0.61\n","======== Epoch 11 / 25 ========\n"," Average training loss: 0.58\n","======== Epoch 12 / 25 ========\n"," Average training loss: 0.54\n","======== Epoch 13 / 25 ========\n"," Average training loss: 0.52\n","======== Epoch 14 / 25 ========\n"," Average training loss: 0.49\n","======== Epoch 15 / 25 ========\n"," Average training loss: 0.47\n","======== Epoch 16 / 25 ========\n"," Average training loss: 0.46\n","======== Epoch 17 / 25 ========\n"," Average training loss: 0.43\n","======== Epoch 18 / 25 ========\n"," Average training loss: 0.42\n","======== Epoch 19 / 25 ========\n"," Average training loss: 0.41\n","======== Epoch 20 / 25 ========\n"," Average training loss: 0.40\n","======== Epoch 21 / 25 ========\n"," Average training loss: 0.39\n","======== Epoch 22 / 25 ========\n"," Average training loss: 0.39\n","======== Epoch 23 / 25 ========\n"," Average training loss: 0.38\n","======== Epoch 24 / 25 ========\n"," Average training loss: 0.38\n","======== Epoch 25 / 25 ========\n"," Average training loss: 0.37\n","Final Validation Accuracy: 0.7053\n","Training complete\n"]}],"source":["import torch\n","import pandas as pd\n","import numpy as np\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.model_selection import train_test_split\n","from torch.utils.data import DataLoader, TensorDataset, RandomSampler\n","from transformers import BertTokenizer, BertModel, AdamW, get_linear_schedule_with_warmup\n","import torch.nn as nn\n","\n","# Load the dataset\n","df = pd.read_csv('/kaggle/input/preclassified/Amex Categorized Raw.csv')\n","df = df[df['Description'] != 'Description']  # Clean any repeated headers\n","\n","# Encode categories\n","label_encoder = LabelEncoder()\n","df['Category_encoded'] = label_encoder.fit_transform(df['Category'])\n","\n","# Initialize the tokenizer\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","\n","# Tokenize the dataset\n","input_ids = []\n","attention_masks = []\n","\n","for desc in df['Description']:\n","    encoded_dict = tokenizer.encode_plus(\n","        desc,                              # Sentence to encode\n","        add_special_tokens = True,         # Add '[CLS]' and '[SEP]'\n","        max_length = 64,                   # Pad & truncate all sentences\n","        pad_to_max_length = True,\n","        return_attention_mask = True,      # Construct attention masks\n","        return_tensors = 'pt',             # Return pytorch tensors\n","    )\n","    \n","    input_ids.append(encoded_dict['input_ids'])\n","    attention_masks.append(encoded_dict['attention_mask'])\n","\n","input_ids = torch.cat(input_ids, dim=0)\n","attention_masks = torch.cat(attention_masks, dim=0)\n","labels = torch.tensor(df['Category_encoded'].values)\n","\n","# Splitting the dataset\n","train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, random_state=42, test_size=0.1)\n","train_masks, validation_masks, _, _ = train_test_split(attention_masks, labels, random_state=42, test_size=0.1)\n","\n","# Convert to DataLoader\n","train_data = TensorDataset(train_inputs, train_masks, train_labels)\n","train_sampler = RandomSampler(train_data)\n","train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=32)\n","\n","validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n","validation_dataloader = DataLoader(validation_data, batch_size=32)\n","\n","\n","# Define the custom model\n","class BertForSequenceClassificationCustom(nn.Module):\n","    def __init__(self, num_labels):\n","        super(BertForSequenceClassificationCustom, self).__init__()\n","        self.num_labels = num_labels\n","        self.bert = BertModel.from_pretrained('bert-base-uncased')\n","        self.dropout = nn.Dropout(0.1)\n","        self.classifier = nn.Linear(768, num_labels)\n","\n","    def forward(self, input_ids, attention_mask=None):\n","        outputs = self.bert(input_ids, attention_mask=attention_mask)\n","        pooled_output = outputs[1]\n","        pooled_output = self.dropout(pooled_output)\n","        logits = self.classifier(pooled_output)\n","        return logits\n","\n","# Initialize the model\n","num_labels = len(df['Category'].unique())\n","model = BertForSequenceClassificationCustom(num_labels)\n","\n","# Define the evaluation function\n","def evaluate_model(model, dataloader, device):\n","    model.eval()\n","    total_eval_accuracy = 0\n","    for batch in dataloader:\n","        batch = tuple(t.to(device) for t in batch)\n","        b_input_ids, b_input_mask, b_labels = batch\n","        \n","        with torch.no_grad():\n","            outputs = model(b_input_ids, attention_mask=b_input_mask)\n","        \n","        logits = outputs.detach().cpu().numpy()\n","        label_ids = b_labels.to('cpu').numpy()\n","\n","        predictions = np.argmax(logits, axis=1).flatten()\n","        labels_flat = label_ids.flatten()\n","\n","        total_eval_accuracy += np.sum(predictions == labels_flat) / len(labels_flat)\n","    \n","    return total_eval_accuracy / len(dataloader)\n","\n","# Before fine-tuning, evaluate the model to get the initial accuracy\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)\n","\n","initial_accuracy = evaluate_model(model, validation_dataloader, device)\n","print(f'Initial Validation Accuracy: {initial_accuracy:.4f}')\n","\n","# Set up optimizer and scheduler\n","optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n","epochs = 25  # Adjust the number of epochs here\n","total_steps = len(train_dataloader) * epochs\n","scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n","\n","# Training loop\n","model.train()\n","for epoch_i in range(0, epochs):  # Loop through 15 epochs\n","    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n","    total_loss = 0\n","\n","    for step, batch in enumerate(train_dataloader):\n","        b_input_ids = batch[0].to(device)\n","        b_input_mask = batch[1].to(device)\n","        b_labels = batch[2].to(device)\n","\n","        model.zero_grad()        \n","\n","        logits = model(b_input_ids, attention_mask=b_input_mask)\n","        \n","        loss_fct = nn.CrossEntropyLoss()\n","        loss = loss_fct(logits, b_labels)\n","        \n","        total_loss += loss.item()\n","        loss.backward()\n","\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","\n","        optimizer.step()\n","        scheduler.step()\n","\n","    avg_train_loss = total_loss / len(train_dataloader)\n","    print(f\" Average training loss: {avg_train_loss:.2f}\")\n","\n","# After fine-tuning, evaluate the model again to get the new accuracy\n","final_accuracy = evaluate_model(model, validation_dataloader, device)\n","print(f'Final Validation Accuracy: {final_accuracy:.4f}')\n","\n","model_save_path = \"/kaggle/working/BERT_BASE_ft_model.pth\"\n","torch.save(model.state_dict(), model_save_path)\n","\n","print(\"Training complete\")\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["## BERT LARGE"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-04-03T01:07:40.965803Z","iopub.status.busy":"2024-04-03T01:07:40.965361Z","iopub.status.idle":"2024-04-03T01:20:57.702877Z","shell.execute_reply":"2024-04-03T01:20:57.701872Z","shell.execute_reply.started":"2024-04-03T01:07:40.965763Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2645: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Initial Validation Accuracy: 0.0000\n","======== Epoch 1 / 25 ========\n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":[" Average training loss: 2.49\n","======== Epoch 2 / 25 ========\n"," Average training loss: 1.73\n","======== Epoch 3 / 25 ========\n"," Average training loss: 1.28\n","======== Epoch 4 / 25 ========\n"," Average training loss: 1.00\n","======== Epoch 5 / 25 ========\n"," Average training loss: 0.84\n","======== Epoch 6 / 25 ========\n"," Average training loss: 0.72\n","======== Epoch 7 / 25 ========\n"," Average training loss: 0.61\n","======== Epoch 8 / 25 ========\n"," Average training loss: 0.56\n","======== Epoch 9 / 25 ========\n"," Average training loss: 0.52\n","======== Epoch 10 / 25 ========\n"," Average training loss: 0.47\n","======== Epoch 11 / 25 ========\n"," Average training loss: 0.42\n","======== Epoch 12 / 25 ========\n"," Average training loss: 0.41\n","======== Epoch 13 / 25 ========\n"," Average training loss: 0.38\n","======== Epoch 14 / 25 ========\n"," Average training loss: 0.36\n","======== Epoch 15 / 25 ========\n"," Average training loss: 0.35\n","======== Epoch 16 / 25 ========\n"," Average training loss: 0.34\n","======== Epoch 17 / 25 ========\n"," Average training loss: 0.33\n","======== Epoch 18 / 25 ========\n"," Average training loss: 0.32\n","======== Epoch 19 / 25 ========\n"," Average training loss: 0.30\n","======== Epoch 20 / 25 ========\n"," Average training loss: 0.29\n","======== Epoch 21 / 25 ========\n"," Average training loss: 0.29\n","======== Epoch 22 / 25 ========\n"," Average training loss: 0.28\n","======== Epoch 23 / 25 ========\n"," Average training loss: 0.28\n","======== Epoch 24 / 25 ========\n"," Average training loss: 0.27\n","======== Epoch 25 / 25 ========\n"," Average training loss: 0.27\n","Final Validation Accuracy: 0.7216\n","Training complete\n"]}],"source":["import torch\n","import pandas as pd\n","import numpy as np\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.model_selection import train_test_split\n","from torch.utils.data import DataLoader, TensorDataset, RandomSampler\n","from transformers import BertTokenizer, BertModel, AdamW, get_linear_schedule_with_warmup\n","import torch.nn as nn\n","\n","# Load the dataset\n","df = pd.read_csv('/kaggle/input/preclassified/Amex Categorized Raw.csv')\n","df = df[df['Description'] != 'Description']  # Clean any repeated headers\n","\n","# Encode categories\n","label_encoder = LabelEncoder()\n","df['Category_encoded'] = label_encoder.fit_transform(df['Category'])\n","\n","# Initialize the tokenizer with bert-large-uncased\n","tokenizer = BertTokenizer.from_pretrained('bert-large-uncased')\n","\n","# Tokenize the dataset\n","input_ids = []\n","attention_masks = []\n","\n","for desc in df['Description']:\n","    encoded_dict = tokenizer.encode_plus(\n","        desc,                              # Sentence to encode\n","        add_special_tokens = True,         # Add '[CLS]' and '[SEP]'\n","        max_length = 64,                   # Pad & truncate all sentences\n","        pad_to_max_length = True,\n","        return_attention_mask = True,      # Construct attention masks\n","        return_tensors = 'pt',             # Return pytorch tensors\n","    )\n","    \n","    input_ids.append(encoded_dict['input_ids'])\n","    attention_masks.append(encoded_dict['attention_mask'])\n","\n","input_ids = torch.cat(input_ids, dim=0)\n","attention_masks = torch.cat(attention_masks, dim=0)\n","labels = torch.tensor(df['Category_encoded'].values)\n","\n","# Splitting the dataset\n","train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, random_state=42, test_size=0.1)\n","train_masks, validation_masks, _, _ = train_test_split(attention_masks, labels, random_state=42, test_size=0.1)\n","\n","# Convert to DataLoader\n","train_data = TensorDataset(train_inputs, train_masks, train_labels)\n","train_sampler = RandomSampler(train_data)\n","train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=32)\n","\n","validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n","validation_dataloader = DataLoader(validation_data, batch_size=32)\n","\n","# Define the custom model with BertModel using bert-large-uncased\n","class BertForSequenceClassificationCustom(nn.Module):\n","    def __init__(self, num_labels):\n","        super(BertForSequenceClassificationCustom, self).__init__()\n","        self.num_labels = num_labels\n","        self.bert = BertModel.from_pretrained('bert-large-uncased')\n","        self.dropout = nn.Dropout(0.1)\n","        self.classifier = nn.Linear(1024, num_labels) # Update the size for bert-large-uncased\n","\n","    def forward(self, input_ids, attention_mask=None):\n","        outputs = self.bert(input_ids, attention_mask=attention_mask)\n","        pooled_output = outputs[1]\n","        pooled_output = self.dropout(pooled_output)\n","        logits = self.classifier(pooled_output)\n","        return logits\n","\n","# Initialize the model\n","num_labels = len(df['Category'].unique())\n","model = BertForSequenceClassificationCustom(num_labels)\n","\n","# (The rest of the code remains unchanged)\n","\n","# Define the evaluation function\n","def evaluate_model(model, dataloader, device):\n","    model.eval()\n","    total_eval_accuracy = 0\n","    for batch in dataloader:\n","        batch = tuple(t.to(device) for t in batch)\n","        b_input_ids, b_input_mask, b_labels = batch\n","        \n","        with torch.no_grad():\n","            outputs = model(b_input_ids, attention_mask=b_input_mask)\n","        \n","        logits = outputs.detach().cpu().numpy()\n","        label_ids = b_labels.to('cpu').numpy()\n","\n","        predictions = np.argmax(logits, axis=1).flatten()\n","        labels_flat = label_ids.flatten()\n","\n","        total_eval_accuracy += np.sum(predictions == labels_flat) / len(labels_flat)\n","    \n","    return total_eval_accuracy / len(dataloader)\n","\n","# Before fine-tuning, evaluate the model to get the initial accuracy\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)\n","\n","initial_accuracy = evaluate_model(model, validation_dataloader, device)\n","print(f'Initial Validation Accuracy: {initial_accuracy:.4f}')\n","\n","# Set up optimizer and scheduler\n","optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n","epochs = 25  # Adjust the number of epochs here\n","total_steps = len(train_dataloader) * epochs\n","scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n","\n","# Training loop\n","model.train()\n","for epoch_i in range(0, epochs):  # Loop through 15 epochs\n","    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n","    total_loss = 0\n","\n","    for step, batch in enumerate(train_dataloader):\n","        b_input_ids = batch[0].to(device)\n","        b_input_mask = batch[1].to(device)\n","        b_labels = batch[2].to(device)\n","\n","        model.zero_grad()        \n","\n","        logits = model(b_input_ids, attention_mask=b_input_mask)\n","        \n","        loss_fct = nn.CrossEntropyLoss()\n","        loss = loss_fct(logits, b_labels)\n","        \n","        total_loss += loss.item()\n","        loss.backward()\n","\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","\n","        optimizer.step()\n","        scheduler.step()\n","\n","    avg_train_loss = total_loss / len(train_dataloader)\n","    print(f\" Average training loss: {avg_train_loss:.2f}\")\n","\n","# After fine-tuning, evaluate the model again to get the new accuracy\n","final_accuracy = evaluate_model(model, validation_dataloader, device)\n","print(f'Final Validation Accuracy: {final_accuracy:.4f}')\n","\n","model_save_path = \"/kaggle/working/BERT_LARGE_ft_model.pth\"\n","torch.save(model.state_dict(), model_save_path)\n","\n","print(\"Training complete\")\n"]},{"cell_type":"markdown","metadata":{},"source":["## Roberta"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-04-03T01:20:57.705004Z","iopub.status.busy":"2024-04-03T01:20:57.704689Z","iopub.status.idle":"2024-04-03T01:34:20.087667Z","shell.execute_reply":"2024-04-03T01:34:20.085144Z","shell.execute_reply.started":"2024-04-03T01:20:57.704964Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2645: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["Initial Validation Accuracy: 0.0078\n","======== Epoch 1 / 25 ========\n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":[" Average training loss: 2.41\n","======== Epoch 2 / 25 ========\n"," Average training loss: 1.61\n","======== Epoch 3 / 25 ========\n"," Average training loss: 1.12\n","======== Epoch 4 / 25 ========\n"," Average training loss: 0.87\n","======== Epoch 5 / 25 ========\n"," Average training loss: 0.71\n","======== Epoch 6 / 25 ========\n"," Average training loss: 0.62\n","======== Epoch 7 / 25 ========\n"," Average training loss: 0.50\n","======== Epoch 8 / 25 ========\n"," Average training loss: 0.46\n","======== Epoch 9 / 25 ========\n"," Average training loss: 0.42\n","======== Epoch 10 / 25 ========\n"," Average training loss: 0.38\n","======== Epoch 11 / 25 ========\n"," Average training loss: 0.36\n","======== Epoch 12 / 25 ========\n"," Average training loss: 0.33\n","======== Epoch 13 / 25 ========\n"," Average training loss: 0.31\n","======== Epoch 14 / 25 ========\n"," Average training loss: 0.30\n","======== Epoch 15 / 25 ========\n"," Average training loss: 0.29\n","======== Epoch 16 / 25 ========\n"," Average training loss: 0.27\n","======== Epoch 17 / 25 ========\n"," Average training loss: 0.25\n","======== Epoch 18 / 25 ========\n"," Average training loss: 0.24\n","======== Epoch 19 / 25 ========\n"," Average training loss: 0.23\n","======== Epoch 20 / 25 ========\n"," Average training loss: 0.22\n","======== Epoch 21 / 25 ========\n"," Average training loss: 0.22\n","======== Epoch 22 / 25 ========\n"," Average training loss: 0.21\n","======== Epoch 23 / 25 ========\n"," Average training loss: 0.21\n","======== Epoch 24 / 25 ========\n"," Average training loss: 0.20\n","======== Epoch 25 / 25 ========\n"," Average training loss: 0.18\n","Final Validation Accuracy: 0.7216\n","Training complete\n"]}],"source":["import torch\n","import pandas as pd\n","import numpy as np\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.model_selection import train_test_split\n","from torch.utils.data import DataLoader, TensorDataset, RandomSampler\n","from transformers import RobertaTokenizer, RobertaModel, AdamW, get_linear_schedule_with_warmup\n","import torch.nn as nn\n","\n","# Load the dataset\n","df = pd.read_csv('/kaggle/input/preclassified/Amex Categorized Raw.csv')\n","df = df[df['Description'] != 'Description']  # Clean any repeated headers\n","\n","# Encode categories\n","label_encoder = LabelEncoder()\n","df['Category_encoded'] = label_encoder.fit_transform(df['Category'])\n","\n","# Initialize the tokenizer with roberta-large\n","tokenizer = RobertaTokenizer.from_pretrained('roberta-large')\n","\n","# Tokenize the dataset\n","input_ids = []\n","attention_masks = []\n","\n","for desc in df['Description']:\n","    encoded_dict = tokenizer.encode_plus(\n","        desc,                              # Sentence to encode\n","        add_special_tokens = True,         # Add '[CLS]' and '[SEP]'\n","        max_length = 64,                   # Pad & truncate all sentences\n","        pad_to_max_length = True,\n","        return_attention_mask = True,      # Construct attention masks\n","        return_tensors = 'pt',             # Return pytorch tensors\n","    )\n","    \n","    input_ids.append(encoded_dict['input_ids'])\n","    attention_masks.append(encoded_dict['attention_mask'])\n","\n","input_ids = torch.cat(input_ids, dim=0)\n","attention_masks = torch.cat(attention_masks, dim=0)\n","labels = torch.tensor(df['Category_encoded'].values)\n","\n","# Splitting the dataset\n","train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, random_state=42, test_size=0.1)\n","train_masks, validation_masks, _, _ = train_test_split(attention_masks, labels, random_state=42, test_size=0.1)\n","\n","# Convert to DataLoader\n","train_data = TensorDataset(train_inputs, train_masks, train_labels)\n","train_sampler = RandomSampler(train_data)\n","train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=32)\n","\n","validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n","validation_dataloader = DataLoader(validation_data, batch_size=32)\n","\n","# Define the custom model with RobertaModel\n","class RobertaForSequenceClassificationCustom(nn.Module):\n","    def __init__(self, num_labels):\n","        super(RobertaForSequenceClassificationCustom, self).__init__()\n","        self.num_labels = num_labels\n","        self.roberta = RobertaModel.from_pretrained('roberta-large')\n","        self.dropout = nn.Dropout(0.1)\n","        self.classifier = nn.Linear(1024, num_labels) # Ensure the size matches roberta-large's output features\n","\n","    def forward(self, input_ids, attention_mask=None):\n","        outputs = self.roberta(input_ids, attention_mask=attention_mask)\n","        pooled_output = outputs[1]\n","        pooled_output = self.dropout(pooled_output)\n","        logits = self.classifier(pooled_output)\n","        return logits\n","\n","# Initialize the model\n","num_labels = len(df['Category'].unique())\n","model = RobertaForSequenceClassificationCustom(num_labels)\n","\n","# (The rest of the code remains unchanged)\n","\n","# Define the evaluation function\n","def evaluate_model(model, dataloader, device):\n","    model.eval()\n","    total_eval_accuracy = 0\n","    for batch in dataloader:\n","        batch = tuple(t.to(device) for t in batch)\n","        b_input_ids, b_input_mask, b_labels = batch\n","        \n","        with torch.no_grad():\n","            outputs = model(b_input_ids, attention_mask=b_input_mask)\n","        \n","        logits = outputs.detach().cpu().numpy()\n","        label_ids = b_labels.to('cpu').numpy()\n","\n","        predictions = np.argmax(logits, axis=1).flatten()\n","        labels_flat = label_ids.flatten()\n","\n","        total_eval_accuracy += np.sum(predictions == labels_flat) / len(labels_flat)\n","    \n","    return total_eval_accuracy / len(dataloader)\n","\n","# Before fine-tuning, evaluate the model to get the initial accuracy\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)\n","\n","initial_accuracy = evaluate_model(model, validation_dataloader, device)\n","print(f'Initial Validation Accuracy: {initial_accuracy:.4f}')\n","\n","# Set up optimizer and scheduler\n","optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n","epochs = 25  # Adjust the number of epochs here\n","total_steps = len(train_dataloader) * epochs\n","scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n","\n","# Training loop\n","model.train()\n","for epoch_i in range(0, epochs):  # Loop through 15 epochs\n","    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n","    total_loss = 0\n","\n","    for step, batch in enumerate(train_dataloader):\n","        b_input_ids = batch[0].to(device)\n","        b_input_mask = batch[1].to(device)\n","        b_labels = batch[2].to(device)\n","\n","        model.zero_grad()        \n","\n","        logits = model(b_input_ids, attention_mask=b_input_mask)\n","        \n","        loss_fct = nn.CrossEntropyLoss()\n","        loss = loss_fct(logits, b_labels)\n","        \n","        total_loss += loss.item()\n","        loss.backward()\n","\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","\n","        optimizer.step()\n","        scheduler.step()\n","\n","    avg_train_loss = total_loss / len(train_dataloader)\n","    print(f\" Average training loss: {avg_train_loss:.2f}\")\n","\n","# After fine-tuning, evaluate the model again to get the new accuracy\n","final_accuracy = evaluate_model(model, validation_dataloader, device)\n","print(f'Final Validation Accuracy: {final_accuracy:.4f}')\n","\n","model_save_path = \"/kaggle/working/RoBERTa_LARGE_ft_model.pth\"\n","torch.save(model.state_dict(), model_save_path)\n","\n","print(\"Training complete\")\n"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":4663241,"sourceId":7933155,"sourceType":"datasetVersion"},{"datasetId":4676648,"sourceId":7952144,"sourceType":"datasetVersion"},{"datasetId":4721437,"sourceId":8014059,"sourceType":"datasetVersion"}],"dockerImageVersionId":30674,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
