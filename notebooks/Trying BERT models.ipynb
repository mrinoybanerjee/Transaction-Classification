{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Trying out different BERT models\n","\n","Through this notebook, my goal was to try out different BERT based models to see which one performs the best. I ended up choosing the BERT-BASE model due to its compact nature and comparable performace to BERT-LEARGE and ROBERTA."]},{"cell_type":"markdown","metadata":{},"source":["## BERT BASE"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-04-13T16:29:31.522178Z","iopub.status.busy":"2024-04-13T16:29:31.521804Z","iopub.status.idle":"2024-04-13T16:31:51.909855Z","shell.execute_reply":"2024-04-13T16:31:51.908776Z","shell.execute_reply.started":"2024-04-13T16:29:31.522149Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2645: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Initial Validation Accuracy: 0.0078\n","======== Epoch 1 / 25 ========\n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":[" Average training loss: 2.49\n","======== Epoch 2 / 25 ========\n"," Average training loss: 2.01\n","======== Epoch 3 / 25 ========\n"," Average training loss: 1.70\n","======== Epoch 4 / 25 ========\n"," Average training loss: 1.45\n","======== Epoch 5 / 25 ========\n"," Average training loss: 1.22\n","======== Epoch 6 / 25 ========\n"," Average training loss: 1.00\n","======== Epoch 7 / 25 ========\n"," Average training loss: 0.81\n","======== Epoch 8 / 25 ========\n"," Average training loss: 0.65\n","======== Epoch 9 / 25 ========\n"," Average training loss: 0.55\n","======== Epoch 10 / 25 ========\n"," Average training loss: 0.46\n","======== Epoch 11 / 25 ========\n"," Average training loss: 0.39\n","======== Epoch 12 / 25 ========\n"," Average training loss: 0.35\n","======== Epoch 13 / 25 ========\n"," Average training loss: 0.30\n","======== Epoch 14 / 25 ========\n"," Average training loss: 0.28\n","======== Epoch 15 / 25 ========\n"," Average training loss: 0.26\n","======== Epoch 16 / 25 ========\n"," Average training loss: 0.22\n","======== Epoch 17 / 25 ========\n"," Average training loss: 0.22\n","======== Epoch 18 / 25 ========\n"," Average training loss: 0.21\n","======== Epoch 19 / 25 ========\n"," Average training loss: 0.20\n","======== Epoch 20 / 25 ========\n"," Average training loss: 0.19\n","======== Epoch 21 / 25 ========\n"," Average training loss: 0.18\n","======== Epoch 22 / 25 ========\n"," Average training loss: 0.18\n","======== Epoch 23 / 25 ========\n"," Average training loss: 0.17\n","======== Epoch 24 / 25 ========\n"," Average training loss: 0.16\n","======== Epoch 25 / 25 ========\n"," Average training loss: 0.16\n","Final Validation Accuracy: 0.8471\n","Training complete\n"]}],"source":["import torch\n","import pandas as pd\n","import numpy as np\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.model_selection import train_test_split\n","from torch.utils.data import DataLoader, TensorDataset, RandomSampler\n","from transformers import BertTokenizer, BertModel, AdamW, get_linear_schedule_with_warmup\n","import torch.nn as nn\n","import pickle\n","\n","# Load the dataset\n","df = pd.read_csv('/kaggle/input/no-negative-transactions/amex_all_categorized_raw.csv')\n","df = df[df['Description'] != 'Description'] \n","\n","# Encode categories\n","label_encoder = LabelEncoder()\n","df['Category_encoded'] = label_encoder.fit_transform(df['Category'])\n","\n","# Save the encoder\n","with open('/kaggle/working/label_encoder.pkl', 'wb') as f:\n","    pickle.dump(label_encoder, f)\n","\n","# Initialize the tokenizer\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","\n","# Tokenize the dataset\n","input_ids = []\n","attention_masks = []\n","\n","for desc in df['Description']:\n","    encoded_dict = tokenizer.encode_plus(\n","        desc,                              # Sentence to encode\n","        add_special_tokens = True,         # Add '[CLS]' and '[SEP]'\n","        max_length = 64,                   # Pad & truncate all sentences\n","        pad_to_max_length = True,\n","        return_attention_mask = True,      # Construct attention masks\n","        return_tensors = 'pt',             # Return pytorch tensors\n","    )\n","    \n","    input_ids.append(encoded_dict['input_ids'])\n","    attention_masks.append(encoded_dict['attention_mask'])\n","\n","input_ids = torch.cat(input_ids, dim=0)\n","attention_masks = torch.cat(attention_masks, dim=0)\n","labels = torch.tensor(df['Category_encoded'].values)\n","\n","# Splitting the dataset\n","train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, random_state=42, test_size=0.1)\n","train_masks, validation_masks, _, _ = train_test_split(attention_masks, labels, random_state=42, test_size=0.1)\n","\n","# Convert to DataLoader\n","train_data = TensorDataset(train_inputs, train_masks, train_labels)\n","train_sampler = RandomSampler(train_data)\n","train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=32)\n","\n","validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n","validation_dataloader = DataLoader(validation_data, batch_size=32)\n","\n","\n","# Define the custom model\n","class BertForSequenceClassificationCustom(nn.Module):\n","    def __init__(self, num_labels):\n","        super(BertForSequenceClassificationCustom, self).__init__()\n","        self.num_labels = num_labels\n","        self.bert = BertModel.from_pretrained('bert-base-uncased')\n","        self.dropout = nn.Dropout(0.1)\n","        self.classifier = nn.Linear(768, num_labels)\n","\n","    def forward(self, input_ids, attention_mask=None):\n","        outputs = self.bert(input_ids, attention_mask=attention_mask)\n","        pooled_output = outputs[1]\n","        pooled_output = self.dropout(pooled_output)\n","        logits = self.classifier(pooled_output)\n","        return logits\n","\n","# Initialize the model\n","num_labels = len(df['Category'].unique())\n","model = BertForSequenceClassificationCustom(num_labels)\n","\n","# Define the evaluation function\n","def evaluate_model(model, dataloader, device):\n","    model.eval()\n","    total_eval_accuracy = 0\n","    for batch in dataloader:\n","        batch = tuple(t.to(device) for t in batch)\n","        b_input_ids, b_input_mask, b_labels = batch\n","        \n","        with torch.no_grad():\n","            outputs = model(b_input_ids, attention_mask=b_input_mask)\n","        \n","        logits = outputs.detach().cpu().numpy()\n","        label_ids = b_labels.to('cpu').numpy()\n","\n","        predictions = np.argmax(logits, axis=1).flatten()\n","        labels_flat = label_ids.flatten()\n","\n","        total_eval_accuracy += np.sum(predictions == labels_flat) / len(labels_flat)\n","    \n","    return total_eval_accuracy / len(dataloader)\n","\n","# Before fine-tuning, evaluate the model to get the initial accuracy\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)\n","\n","initial_accuracy = evaluate_model(model, validation_dataloader, device)\n","print(f'Initial Validation Accuracy: {initial_accuracy:.4f}')\n","\n","# Set up optimizer and scheduler\n","optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n","epochs = 25  # Adjust the number of epochs here\n","total_steps = len(train_dataloader) * epochs\n","scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n","\n","# Training loop\n","model.train()\n","for epoch_i in range(0, epochs):  # Loop through 15 epochs\n","    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n","    total_loss = 0\n","\n","    for step, batch in enumerate(train_dataloader):\n","        b_input_ids = batch[0].to(device)\n","        b_input_mask = batch[1].to(device)\n","        b_labels = batch[2].to(device)\n","\n","        model.zero_grad()        \n","\n","        logits = model(b_input_ids, attention_mask=b_input_mask)\n","        \n","        loss_fct = nn.CrossEntropyLoss()\n","        loss = loss_fct(logits, b_labels)\n","        \n","        total_loss += loss.item()\n","        loss.backward()\n","\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","\n","        optimizer.step()\n","        scheduler.step()\n","\n","    avg_train_loss = total_loss / len(train_dataloader)\n","    print(f\" Average training loss: {avg_train_loss:.2f}\")\n","\n","# After fine-tuning, evaluate the model again to get the new accuracy\n","final_accuracy = evaluate_model(model, validation_dataloader, device)\n","print(f'Final Validation Accuracy: {final_accuracy:.4f}')\n","\n","model_save_path = \"/kaggle/working/BERT_BASE_ft_model.pth\"\n","torch.save(model.state_dict(), model_save_path)\n","\n","print(\"Training complete\")\n"]},{"cell_type":"markdown","metadata":{},"source":["## BERT LARGE"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-04-13T16:09:53.740274Z","iopub.status.busy":"2024-04-13T16:09:53.739924Z","iopub.status.idle":"2024-04-13T16:18:09.829501Z","shell.execute_reply":"2024-04-13T16:18:09.828433Z","shell.execute_reply.started":"2024-04-13T16:09:53.740243Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c5fabacbe6f84999860cf44a69ade6b7","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5ad2bea782b2476ea7537667322661ab","version_major":2,"version_minor":0},"text/plain":["vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"02d698e08e434d1a80fd49608f856bff","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5de13aea355740cf93b49595f6ad8484","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2645: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"48ef8f2f37f84a5da28815c7813828d6","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/1.34G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Initial Validation Accuracy: 0.0625\n","======== Epoch 1 / 25 ========\n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":[" Average training loss: 2.42\n","======== Epoch 2 / 25 ========\n"," Average training loss: 2.05\n","======== Epoch 3 / 25 ========\n"," Average training loss: 1.63\n","======== Epoch 4 / 25 ========\n"," Average training loss: 1.15\n","======== Epoch 5 / 25 ========\n"," Average training loss: 0.86\n","======== Epoch 6 / 25 ========\n"," Average training loss: 0.66\n","======== Epoch 7 / 25 ========\n"," Average training loss: 0.49\n","======== Epoch 8 / 25 ========\n"," Average training loss: 0.41\n","======== Epoch 9 / 25 ========\n"," Average training loss: 0.34\n","======== Epoch 10 / 25 ========\n"," Average training loss: 0.27\n","======== Epoch 11 / 25 ========\n"," Average training loss: 0.25\n","======== Epoch 12 / 25 ========\n"," Average training loss: 0.21\n","======== Epoch 13 / 25 ========\n"," Average training loss: 0.19\n","======== Epoch 14 / 25 ========\n"," Average training loss: 0.19\n","======== Epoch 15 / 25 ========\n"," Average training loss: 0.18\n","======== Epoch 16 / 25 ========\n"," Average training loss: 0.16\n","======== Epoch 17 / 25 ========\n"," Average training loss: 0.16\n","======== Epoch 18 / 25 ========\n"," Average training loss: 0.14\n","======== Epoch 19 / 25 ========\n"," Average training loss: 0.13\n","======== Epoch 20 / 25 ========\n"," Average training loss: 0.14\n","======== Epoch 21 / 25 ========\n"," Average training loss: 0.13\n","======== Epoch 22 / 25 ========\n"," Average training loss: 0.12\n","======== Epoch 23 / 25 ========\n"," Average training loss: 0.11\n","======== Epoch 24 / 25 ========\n"," Average training loss: 0.11\n","======== Epoch 25 / 25 ========\n"," Average training loss: 0.11\n","Final Validation Accuracy: 0.8471\n","Training complete\n"]}],"source":["import torch\n","import pandas as pd\n","import numpy as np\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.model_selection import train_test_split\n","from torch.utils.data import DataLoader, TensorDataset, RandomSampler\n","from transformers import BertTokenizer, BertModel, AdamW, get_linear_schedule_with_warmup\n","import torch.nn as nn\n","\n","# Load the dataset\n","df = pd.read_csv('/kaggle/input/no-negative-transactions/amex_all_categorized_raw.csv')\n","df = df[df['Description'] != 'Description']\n","\n","# Encode categories\n","label_encoder = LabelEncoder()\n","df['Category_encoded'] = label_encoder.fit_transform(df['Category'])\n","\n","# Initialize the tokenizer with bert-large-uncased\n","tokenizer = BertTokenizer.from_pretrained('bert-large-uncased')\n","\n","# Tokenize the dataset\n","input_ids = []\n","attention_masks = []\n","\n","for desc in df['Description']:\n","    encoded_dict = tokenizer.encode_plus(\n","        desc,                              # Sentence to encode\n","        add_special_tokens = True,         # Add '[CLS]' and '[SEP]'\n","        max_length = 64,                   # Pad & truncate all sentences\n","        pad_to_max_length = True,\n","        return_attention_mask = True,      # Construct attention masks\n","        return_tensors = 'pt',             # Return pytorch tensors\n","    )\n","    \n","    input_ids.append(encoded_dict['input_ids'])\n","    attention_masks.append(encoded_dict['attention_mask'])\n","\n","input_ids = torch.cat(input_ids, dim=0)\n","attention_masks = torch.cat(attention_masks, dim=0)\n","labels = torch.tensor(df['Category_encoded'].values)\n","\n","# Splitting the dataset\n","train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, random_state=42, test_size=0.1)\n","train_masks, validation_masks, _, _ = train_test_split(attention_masks, labels, random_state=42, test_size=0.1)\n","\n","# Convert to DataLoader\n","train_data = TensorDataset(train_inputs, train_masks, train_labels)\n","train_sampler = RandomSampler(train_data)\n","train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=32)\n","\n","validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n","validation_dataloader = DataLoader(validation_data, batch_size=32)\n","\n","# Define the custom model with BertModel using bert-large-uncased\n","class BertForSequenceClassificationCustom(nn.Module):\n","    def __init__(self, num_labels):\n","        super(BertForSequenceClassificationCustom, self).__init__()\n","        self.num_labels = num_labels\n","        self.bert = BertModel.from_pretrained('bert-large-uncased')\n","        self.dropout = nn.Dropout(0.1)\n","        self.classifier = nn.Linear(1024, num_labels) # Update the size for bert-large-uncased\n","\n","    def forward(self, input_ids, attention_mask=None):\n","        outputs = self.bert(input_ids, attention_mask=attention_mask)\n","        pooled_output = outputs[1]\n","        pooled_output = self.dropout(pooled_output)\n","        logits = self.classifier(pooled_output)\n","        return logits\n","\n","# Initialize the model\n","num_labels = len(df['Category'].unique())\n","model = BertForSequenceClassificationCustom(num_labels)\n","\n","# (The rest of the code remains unchanged)\n","\n","# Define the evaluation function\n","def evaluate_model(model, dataloader, device):\n","    model.eval()\n","    total_eval_accuracy = 0\n","    for batch in dataloader:\n","        batch = tuple(t.to(device) for t in batch)\n","        b_input_ids, b_input_mask, b_labels = batch\n","        \n","        with torch.no_grad():\n","            outputs = model(b_input_ids, attention_mask=b_input_mask)\n","        \n","        logits = outputs.detach().cpu().numpy()\n","        label_ids = b_labels.to('cpu').numpy()\n","\n","        predictions = np.argmax(logits, axis=1).flatten()\n","        labels_flat = label_ids.flatten()\n","\n","        total_eval_accuracy += np.sum(predictions == labels_flat) / len(labels_flat)\n","    \n","    return total_eval_accuracy / len(dataloader)\n","\n","# Before fine-tuning, evaluate the model to get the initial accuracy\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)\n","\n","initial_accuracy = evaluate_model(model, validation_dataloader, device)\n","print(f'Initial Validation Accuracy: {initial_accuracy:.4f}')\n","\n","# Set up optimizer and scheduler\n","optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n","epochs = 25  # Adjust the number of epochs here\n","total_steps = len(train_dataloader) * epochs\n","scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n","\n","# Training loop\n","model.train()\n","for epoch_i in range(0, epochs):  # Loop through 15 epochs\n","    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n","    total_loss = 0\n","\n","    for step, batch in enumerate(train_dataloader):\n","        b_input_ids = batch[0].to(device)\n","        b_input_mask = batch[1].to(device)\n","        b_labels = batch[2].to(device)\n","\n","        model.zero_grad()        \n","\n","        logits = model(b_input_ids, attention_mask=b_input_mask)\n","        \n","        loss_fct = nn.CrossEntropyLoss()\n","        loss = loss_fct(logits, b_labels)\n","        \n","        total_loss += loss.item()\n","        loss.backward()\n","\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","\n","        optimizer.step()\n","        scheduler.step()\n","\n","    avg_train_loss = total_loss / len(train_dataloader)\n","    print(f\" Average training loss: {avg_train_loss:.2f}\")\n","\n","# After fine-tuning, evaluate the model again to get the new accuracy\n","final_accuracy = evaluate_model(model, validation_dataloader, device)\n","print(f'Final Validation Accuracy: {final_accuracy:.4f}')\n","\n","model_save_path = \"/kaggle/working/BERT_LARGE_ft_model.pth\"\n","torch.save(model.state_dict(), model_save_path)\n","\n","print(\"Training complete\")\n"]},{"cell_type":"markdown","metadata":{},"source":["## Roberta"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-04-13T16:18:09.831696Z","iopub.status.busy":"2024-04-13T16:18:09.831377Z","iopub.status.idle":"2024-04-13T16:26:28.882240Z","shell.execute_reply":"2024-04-13T16:26:28.881133Z","shell.execute_reply.started":"2024-04-13T16:18:09.831669Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7f10cf503ee743bab5e5d8ffa203c4f3","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2e97b012352b458aadef42694e95d2b4","version_major":2,"version_minor":0},"text/plain":["vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f51b76fd2da441388957d9d08e29a632","version_major":2,"version_minor":0},"text/plain":["merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7fbe9e3b1e1649c9b6a00a057b99ebd8","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"fc889dd17d614fcaa380436baaa3b98e","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2645: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d24bc6c7e7694bed89d9672a752f1958","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["Initial Validation Accuracy: 0.0703\n","======== Epoch 1 / 25 ========\n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":[" Average training loss: 2.28\n","======== Epoch 2 / 25 ========\n"," Average training loss: 1.57\n","======== Epoch 3 / 25 ========\n"," Average training loss: 0.96\n","======== Epoch 4 / 25 ========\n"," Average training loss: 0.67\n","======== Epoch 5 / 25 ========\n"," Average training loss: 0.51\n","======== Epoch 6 / 25 ========\n"," Average training loss: 0.38\n","======== Epoch 7 / 25 ========\n"," Average training loss: 0.32\n","======== Epoch 8 / 25 ========\n"," Average training loss: 0.28\n","======== Epoch 9 / 25 ========\n"," Average training loss: 0.24\n","======== Epoch 10 / 25 ========\n"," Average training loss: 0.22\n","======== Epoch 11 / 25 ========\n"," Average training loss: 0.18\n","======== Epoch 12 / 25 ========\n"," Average training loss: 0.18\n","======== Epoch 13 / 25 ========\n"," Average training loss: 0.17\n","======== Epoch 14 / 25 ========\n"," Average training loss: 0.17\n","======== Epoch 15 / 25 ========\n"," Average training loss: 0.16\n","======== Epoch 16 / 25 ========\n"," Average training loss: 0.14\n","======== Epoch 17 / 25 ========\n"," Average training loss: 0.13\n","======== Epoch 18 / 25 ========\n"," Average training loss: 0.13\n","======== Epoch 19 / 25 ========\n"," Average training loss: 0.13\n","======== Epoch 20 / 25 ========\n"," Average training loss: 0.12\n","======== Epoch 21 / 25 ========\n"," Average training loss: 0.11\n","======== Epoch 22 / 25 ========\n"," Average training loss: 0.10\n","======== Epoch 23 / 25 ========\n"," Average training loss: 0.10\n","======== Epoch 24 / 25 ========\n"," Average training loss: 0.10\n","======== Epoch 25 / 25 ========\n"," Average training loss: 0.10\n","Final Validation Accuracy: 0.8549\n","Training complete\n"]}],"source":["import torch\n","import pandas as pd\n","import numpy as np\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.model_selection import train_test_split\n","from torch.utils.data import DataLoader, TensorDataset, RandomSampler\n","from transformers import RobertaTokenizer, RobertaModel, AdamW, get_linear_schedule_with_warmup\n","import torch.nn as nn\n","\n","# Load the dataset\n","df = pd.read_csv('/kaggle/input/no-negative-transactions/amex_all_categorized_raw.csv')\n","df = df[df['Description'] != 'Description']  # Clean any repeated headers\n","\n","# Encode categories\n","label_encoder = LabelEncoder()\n","df['Category_encoded'] = label_encoder.fit_transform(df['Category'])\n","\n","# Initialize the tokenizer with roberta-large\n","tokenizer = RobertaTokenizer.from_pretrained('roberta-large')\n","\n","# Tokenize the dataset\n","input_ids = []\n","attention_masks = []\n","\n","for desc in df['Description']:\n","    encoded_dict = tokenizer.encode_plus(\n","        desc,                              # Sentence to encode\n","        add_special_tokens = True,         # Add '[CLS]' and '[SEP]'\n","        max_length = 64,                   # Pad & truncate all sentences\n","        pad_to_max_length = True,\n","        return_attention_mask = True,      # Construct attention masks\n","        return_tensors = 'pt',             # Return pytorch tensors\n","    )\n","    \n","    input_ids.append(encoded_dict['input_ids'])\n","    attention_masks.append(encoded_dict['attention_mask'])\n","\n","input_ids = torch.cat(input_ids, dim=0)\n","attention_masks = torch.cat(attention_masks, dim=0)\n","labels = torch.tensor(df['Category_encoded'].values)\n","\n","# Splitting the dataset\n","train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, random_state=42, test_size=0.1)\n","train_masks, validation_masks, _, _ = train_test_split(attention_masks, labels, random_state=42, test_size=0.1)\n","\n","# Convert to DataLoader\n","train_data = TensorDataset(train_inputs, train_masks, train_labels)\n","train_sampler = RandomSampler(train_data)\n","train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=32)\n","\n","validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n","validation_dataloader = DataLoader(validation_data, batch_size=32)\n","\n","# Define the custom model with RobertaModel\n","class RobertaForSequenceClassificationCustom(nn.Module):\n","    def __init__(self, num_labels):\n","        super(RobertaForSequenceClassificationCustom, self).__init__()\n","        self.num_labels = num_labels\n","        self.roberta = RobertaModel.from_pretrained('roberta-large')\n","        self.dropout = nn.Dropout(0.1)\n","        self.classifier = nn.Linear(1024, num_labels) # Ensure the size matches roberta-large's output features\n","\n","    def forward(self, input_ids, attention_mask=None):\n","        outputs = self.roberta(input_ids, attention_mask=attention_mask)\n","        pooled_output = outputs[1]\n","        pooled_output = self.dropout(pooled_output)\n","        logits = self.classifier(pooled_output)\n","        return logits\n","\n","# Initialize the model\n","num_labels = len(df['Category'].unique())\n","model = RobertaForSequenceClassificationCustom(num_labels)\n","\n","# Define the evaluation function\n","def evaluate_model(model, dataloader, device):\n","    model.eval()\n","    total_eval_accuracy = 0\n","    for batch in dataloader:\n","        batch = tuple(t.to(device) for t in batch)\n","        b_input_ids, b_input_mask, b_labels = batch\n","        \n","        with torch.no_grad():\n","            outputs = model(b_input_ids, attention_mask=b_input_mask)\n","        \n","        logits = outputs.detach().cpu().numpy()\n","        label_ids = b_labels.to('cpu').numpy()\n","\n","        predictions = np.argmax(logits, axis=1).flatten()\n","        labels_flat = label_ids.flatten()\n","\n","        total_eval_accuracy += np.sum(predictions == labels_flat) / len(labels_flat)\n","    \n","    return total_eval_accuracy / len(dataloader)\n","\n","# Before fine-tuning, evaluate the model to get the initial accuracy\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)\n","\n","initial_accuracy = evaluate_model(model, validation_dataloader, device)\n","print(f'Initial Validation Accuracy: {initial_accuracy:.4f}')\n","\n","# Set up optimizer and scheduler\n","optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n","epochs = 25  # Adjust the number of epochs here\n","total_steps = len(train_dataloader) * epochs\n","scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n","\n","# Training loop\n","model.train()\n","for epoch_i in range(0, epochs):  # Loop through 15 epochs\n","    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n","    total_loss = 0\n","\n","    for step, batch in enumerate(train_dataloader):\n","        b_input_ids = batch[0].to(device)\n","        b_input_mask = batch[1].to(device)\n","        b_labels = batch[2].to(device)\n","\n","        model.zero_grad()        \n","\n","        logits = model(b_input_ids, attention_mask=b_input_mask)\n","        \n","        loss_fct = nn.CrossEntropyLoss()\n","        loss = loss_fct(logits, b_labels)\n","        \n","        total_loss += loss.item()\n","        loss.backward()\n","\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","\n","        optimizer.step()\n","        scheduler.step()\n","\n","    avg_train_loss = total_loss / len(train_dataloader)\n","    print(f\" Average training loss: {avg_train_loss:.2f}\")\n","\n","# After fine-tuning, evaluate the model again to get the new accuracy\n","final_accuracy = evaluate_model(model, validation_dataloader, device)\n","print(f'Final Validation Accuracy: {final_accuracy:.4f}')\n","\n","model_save_path = \"/kaggle/working/RoBERTa_LARGE_ft_model.pth\"\n","torch.save(model.state_dict(), model_save_path)\n","\n","print(\"Training complete\")\n"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":4663241,"sourceId":7933155,"sourceType":"datasetVersion"},{"datasetId":4676648,"sourceId":7952144,"sourceType":"datasetVersion"},{"datasetId":4721437,"sourceId":8014059,"sourceType":"datasetVersion"},{"datasetId":4786008,"sourceId":8103784,"sourceType":"datasetVersion"},{"datasetId":4790545,"sourceId":8109906,"sourceType":"datasetVersion"}],"dockerImageVersionId":30674,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.7"}},"nbformat":4,"nbformat_minor":4}
